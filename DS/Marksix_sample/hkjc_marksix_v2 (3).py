# -*- coding: utf-8 -*-
"""HKJC_marksix_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dvNpqr14xcn_Lh8Zq7j3rdo7kTdboT57
"""

!pip install scikit-optimize

#!/usr/bin/env python
import cvxpy as cp
import numpy as np
import pandas as pd
from scipy.stats import ttest_1samp
from itertools import combinations
from pyspark.sql import SparkSession
from pyspark.ml.fpm import FPGrowth
from skopt import gp_minimize
from skopt.space import Real, Integer
from skopt.utils import use_named_args
from tqdm import tqdm  # For progress bar

# ------------------------------
# Fixed Parameters (Constants)
# ------------------------------
portfolio_size = 100       # number of lottery tickets
ticket_length = 6          # each ticket selects 6 numbers
num_range = 49             # numbers from 1 to 49
ticket_cost = 10           # cost per ticket
prize_ref = 8000000        # Division 1 prize (if all 6 match)

# ------------------------------
# Custom tqdm Callback for Bayesian Optimization
# ------------------------------
class TqdmCallback:
    def __init__(self, total):
        self.pbar = tqdm(total=total, desc="Bayesian Optimization", unit="eval")
    def __call__(self, res):
        self.pbar.update(1)
    def __del__(self):
        self.pbar.close()

# ------------------------------
# Helper Functions for Data and Pattern Mining
# ------------------------------
def compute_individual_frequency(data, num_range=49):
    winning_numbers = data[[str(i) for i in range(1, 7)]].values.flatten()
    counts = np.bincount(winning_numbers - 1, minlength=num_range)
    weights = counts / counts.sum()
    return weights

def mine_frequent_patterns_spark(data, min_support=0.0001):
    spark = SparkSession.builder.appName("FPGrowthLottery").getOrCreate()
    transactions = data[[str(i) for i in range(1, 7)]].values.tolist()
    df = spark.createDataFrame([(t,) for t in transactions], ["items"])
    fpGrowth = FPGrowth(itemsCol="items", minSupport=min_support, minConfidence=0.0)
    model = fpGrowth.fit(df)
    freqItemsets = model.freqItemsets.collect()
    N = len(transactions)
    patterns = []
    for row in freqItemsets:
        items = row['items']
        if len(items) >= 2:
            p_prob = (row['freq'] + 1) / (N + 2)
            patterns.append({'itemset': sorted(items), 'p_prob': p_prob})
    spark.stop()
    return patterns

# ------------------------------
# Enhanced cvxpy Problem Formulation with Risk-Aware Objective
# ------------------------------
def solve_optimization(a, patterns, lambda_pattern_val, diversification_limit_val, risk_aversion, diversification_penalty):
    # Boost the frequency weights with the mined patterns:
    b = np.array(a, dtype=float)
    for pattern in patterns:
        for num in pattern['itemset']:
            b[num-1] += lambda_pattern_val * pattern['p_prob'] / len(pattern['itemset'])

    # Define the decision variable X (each row is one ticket)
    X = cp.Variable((portfolio_size, num_range))
    col_sum = cp.sum(X, axis=0)
    # New risk-aware objective:
    #   risk_aversion * ||X||_F^2 + diversification_penalty * ||col_sum||_2^2 - dot(b, col_sum)
    objective = cp.Minimize(risk_aversion * cp.sum(cp.square(X)) +
                            diversification_penalty * cp.sum(cp.square(col_sum)) -
                            cp.sum(cp.multiply(b, col_sum)))
    constraints = [
        cp.sum(X, axis=1) == ticket_length,
        cp.sum(X, axis=0) <= diversification_limit_val,
        X >= 0,
        X <= 1
    ]
    problem = cp.Problem(objective, constraints)
    solvers = [(cp.SCS, {"eps": 1e-8}), (cp.OSQP, {}), (cp.ECOS, {})]
    solution = None
    for solver, options in solvers:
        try:
            problem.solve(solver=solver, **options, verbose=False)
            if problem.status in ["optimal", "optimal_inaccurate"]:
                solution = X.value
                break
            elif X.value is not None:
                solution = X.value
        except Exception as e:
            pass
    if solution is None:
        print("Warning: No solution was found; returning a zero array.")
        solution = np.zeros((portfolio_size, num_range))
    return solution

# ------------------------------
# Improved Rounding and Diversification using Probabilistic Rounding
# ------------------------------
def round_and_diversify_improved_v2(X_opt, top_k_multiplier=20, max_attempts=10, temperature=0.1):
    # Use softmax-based rounding for each ticket (row of X_opt)
    discrete_portfolio = []
    for i in range(X_opt.shape[0]):
        row = X_opt[i, :]
        # Compute softmax probabilities (subtract max for stability)
        max_val = np.max(row)
        exp_vals = np.exp((row - max_val) / temperature)
        probs = exp_vals / np.sum(exp_vals)
        # Sample ticket_length numbers without replacement using these probabilities
        ticket = np.random.choice(np.arange(1, num_range+1), size=ticket_length, replace=False, p=probs)
        discrete_portfolio.append(sorted(ticket.tolist()))

    # Try to diversify duplicate tickets
    diversified = discrete_portfolio.copy()
    attempt = 0
    while attempt < max_attempts:
        seen = set()
        duplicates = []
        for i, ticket in enumerate(diversified):
            ttuple = tuple(ticket)
            if ttuple in seen:
                duplicates.append(i)
            else:
                seen.add(ttuple)
        if not duplicates:
            break
        for i in duplicates:
            row = X_opt[i, :]
            candidate_pool_size = min(top_k_multiplier * ticket_length, num_range)
            candidate_indices = np.argpartition(row, -candidate_pool_size)[-candidate_pool_size:]
            candidate_indices = np.sort(candidate_indices)
            for comb in combinations(candidate_indices, ticket_length):
                new_ticket = sorted([int(num)+1 for num in comb])
                if tuple(new_ticket) not in seen:
                    diversified[i] = new_ticket
                    seen.add(tuple(new_ticket))
                    break
        attempt += 1
    return diversified

# ------------------------------
# Postprocessing for Global Occurrence and Pairwise Diversity
# ------------------------------
def postprocess_diversity(portfolio, X_opt, max_occurrence=5):
    occurrences = {}
    for ticket in portfolio:
        for num in ticket:
            occurrences[num] = occurrences.get(num, 0) + 1
    new_portfolio = portfolio.copy()
    for i, ticket in enumerate(new_portfolio):
        for j, num in enumerate(ticket):
            if occurrences[num] > max_occurrence:
                row = X_opt[i, :]
                candidate_indices = np.argsort(-row)
                for cand in candidate_indices:
                    cand_num = int(cand) + 1
                    if cand_num not in ticket and occurrences.get(cand_num, 0) < max_occurrence:
                        occurrences[num] -= 1
                        ticket[j] = cand_num
                        occurrences[cand_num] = occurrences.get(cand_num, 0) + 1
                        break
        new_portfolio[i] = sorted(ticket)
    return new_portfolio

def reduce_max_pair_overlap(portfolio, max_pair_overlap=2, max_iter=100):
    # Greedily adjust tickets to reduce pairwise overlap
    portfolio_sets = [set(ticket) for ticket in portfolio]
    def total_overlap(idx, candidate_set):
        tot = 0
        for j, s in enumerate(portfolio_sets):
            if j != idx:
                tot += len(candidate_set & s)
        return tot
    for it in range(max_iter):
        changed = False
        for i in range(len(portfolio_sets)):
            for j in range(i+1, len(portfolio_sets)):
                common = portfolio_sets[i] & portfolio_sets[j]
                if len(common) > max_pair_overlap:
                    current_set = portfolio_sets[j]
                    current_overlap = total_overlap(j, current_set)
                    best_set = current_set
                    best_overlap = current_overlap
                    for num in list(common):
                        for cand in range(1, num_range+1):
                            if cand in current_set:
                                continue
                            candidate_set = (current_set - {num}) | {cand}
                            new_overlap = total_overlap(j, candidate_set)
                            if new_overlap < best_overlap:
                                best_overlap = new_overlap
                                best_set = candidate_set
                        if best_overlap < current_overlap:
                            portfolio_sets[j] = best_set
                            changed = True
                            break
                    if changed:
                        break
            if changed:
                break
        if not changed:
            break
    return [sorted(list(s)) for s in portfolio_sets]

def postprocess_diversity_v3(portfolio, X_opt, max_occurrence=5, max_pair_overlap=2):
    portfolio_processed = postprocess_diversity(portfolio, X_opt, max_occurrence)
    diversified_portfolio = reduce_max_pair_overlap(portfolio_processed, max_pair_overlap, max_iter=100)
    return diversified_portfolio

# ------------------------------
# CPU-Based Simulation / Backtesting Function
# ------------------------------
def simulate_profit_cpu(portfolio, draws_main, draws_extra, prize_ref, ticket_cost):
    num_tickets = len(portfolio)
    n_draws = draws_main.shape[0]
    total_cost = num_tickets * ticket_cost

    # Create ticket matrix T: shape (num_tickets, num_range)
    T = np.zeros((num_tickets, num_range), dtype=int)
    for i, ticket in enumerate(portfolio):
        indices = [num - 1 for num in ticket]
        T[i, indices] = 1

    # Create draw matrix D: shape (n_draws, num_range)
    D = np.zeros((n_draws, num_range), dtype=int)
    for i in range(n_draws):
        numbers = draws_main[i] - 1  # convert to 0-indexed
        D[i, numbers] = 1

    # Compute matches
    matches = D.dot(T.T)

    # Extra matching matrix E: shape (n_draws, num_tickets)
    E = np.zeros((n_draws, num_tickets), dtype=int)
    for i in range(n_draws):
        extra = draws_extra[i] - 1
        E[i, :] = T[:, extra]

    # Compute rewards R based on matching criteria
    R = np.zeros_like(matches, dtype=float)
    R += (matches == 6) * prize_ref
    R += (matches == 5) * np.where(E == 1, 0.15 * prize_ref, 0.10 * prize_ref)
    R += (matches == 4) * np.where(E == 1, 9600, 640)
    R += (matches == 3) * np.where(E == 1, 320, 40)

    revenue = np.sum(R, axis=1)
    profits = revenue - total_cost
    total_profit = np.sum(profits)
    mean_profit = np.mean(profits)
    variance = np.sum((profits - mean_profit)**2) / (n_draws - 1)

    return profits, total_profit, mean_profit, variance

# ------------------------------
# Validation Evaluation Function to Mitigate Overfitting
# ------------------------------
def evaluate_parameters_validation(a, patterns, data_val, lambda_pattern_val, diversification_limit_val, max_occurrence, max_pair_overlap, risk_aversion, diversification_penalty):
    X_opt = solve_optimization(a, patterns, lambda_pattern_val, diversification_limit_val, risk_aversion, diversification_penalty)
    portfolio = round_and_diversify_improved_v2(X_opt, top_k_multiplier=20, max_attempts=10, temperature=0.1)
    portfolio = postprocess_diversity_v3(portfolio, X_opt, max_occurrence=max_occurrence, max_pair_overlap=max_pair_overlap)
    draws_val_main = data_val.iloc[:, :6].values
    draws_val_extra = data_val.iloc[:, 6].values
    profits, tot_profit, mean_profit, var_profit = simulate_profit_cpu(portfolio, draws_val_main, draws_val_extra, prize_ref, ticket_cost)
    return profits, tot_profit, mean_profit, var_profit, portfolio

# ------------------------------
# Final Evaluation on Full Training Data (for Final Portfolio Generation)
# ------------------------------
def evaluate_parameters_training(a, patterns, data_train, lambda_pattern_val, diversification_limit_val, max_occurrence, max_pair_overlap, risk_aversion, diversification_penalty):
    X_opt = solve_optimization(a, patterns, lambda_pattern_val, diversification_limit_val, risk_aversion, diversification_penalty)
    portfolio = round_and_diversify_improved_v2(X_opt, top_k_multiplier=20, max_attempts=10, temperature=0.1)
    portfolio = postprocess_diversity_v3(portfolio, X_opt, max_occurrence=max_occurrence, max_pair_overlap=max_pair_overlap)
    draws_train_main = data_train.iloc[:, :6].values
    draws_train_extra = data_train.iloc[:, 6].values
    profits, tot_profit, mean_profit, var_profit = simulate_profit_cpu(portfolio, draws_train_main, draws_train_extra, prize_ref, ticket_cost)
    return profits, tot_profit, mean_profit, var_profit, portfolio

# ------------------------------
# Main: Data Loading, Optimization, and Testing
# ------------------------------
def main():
    try:
        data = pd.read_csv("Mark_Six.csv")
    except Exception as e:
        print("Error loading CSV file:", e)
        return
    data['Date'] = pd.to_datetime(data['Date'])
    data = data.sort_values(by='Date').reset_index(drop=True)
    data = data[['Date', 'Winning Number 1', '2', '3', '4', '5', '6', 'Extra Number']]
    data = data[data['Date'] >= '2010-11-09'].reset_index(drop=True)
    data = data.drop('Date', axis=1)
    data.columns = [str(i) for i in range(1, 8)]
    data = data.astype(int)

    n_total = data.shape[0]
    n_train = int(n_total * 0.5)
    data_train_full = data.iloc[:n_train].reset_index(drop=True)
    data_test = data.iloc[n_train:].reset_index(drop=True)
    print(f"Total draws: {n_total}; Training draws: {data_train_full.shape[0]}; Testing draws: {data_test.shape[0]}")

    # Split training data into optimization and validation subsets (80/20 split)
    n_train_opt = int(data_train_full.shape[0] * 0.8)
    data_train_opt = data_train_full.iloc[:n_train_opt].reset_index(drop=True)
    data_val = data_train_full.iloc[n_train_opt:].reset_index(drop=True)
    print(f"Optimization training draws: {data_train_opt.shape[0]}; Validation draws: {data_val.shape[0]}")

    a = compute_individual_frequency(data_train_opt, num_range=num_range)
    print("Mining frequent patterns using Spark’s MLlib FP‑Growth on training subset...")
    patterns = mine_frequent_patterns_spark(data_train_opt, min_support=1e-32)

    # Define a 6-dimensional hyperparameter search space:
    space  = [
        Real(0.0, 10.0, name='lambda_pattern_val'),
        Real(0.13, 1.0, name='diversification_multiplier'),
        Integer(1, portfolio_size, name='max_occurrence'),
        Integer(0, ticket_length, name='max_pair_overlap'),
        Real(1e-4, 1e-1, name='risk_aversion'),
        Real(1e-4, 1e-1, name='diversification_penalty')
    ]

    @use_named_args(space)
    def objective(**params):
        lambda_pattern_val = params['lambda_pattern_val']
        div_mult = params['diversification_multiplier']
        max_occurrence = params['max_occurrence']
        max_pair_overlap = params['max_pair_overlap']
        risk_aversion = params['risk_aversion']
        diversification_penalty = params['diversification_penalty']
        diversification_limit_val = portfolio_size * div_mult
        try:
            profits, tot_profit, mean_profit, var_profit, _ = evaluate_parameters_validation(
                a, patterns, data_val, lambda_pattern_val, diversification_limit_val, max_occurrence, max_pair_overlap, risk_aversion, diversification_penalty)
            n = len(profits)
            std_profit = np.sqrt(var_profit + 1e-8)
            t_stat = mean_profit / (std_profit / np.sqrt(n))
            print(f"Evaluated: lambda={lambda_pattern_val:.3f}, div_mult={div_mult:.3f}, max_occurrence={max_occurrence}, max_pair_overlap={max_pair_overlap}, risk_aversion={risk_aversion:.4f}, diversification_penalty={diversification_penalty:.4f} => t-stat (validation) = {t_stat:.4f}")
            return -t_stat
        except Exception as e:
            print(f"Evaluation failed for params: {params} with error: {e}")
            return 1e12

    print("\nStarting Bayesian optimization on validation data (optimizing t-statistic)...")
    # Set the total number of calls for the progress bar
    n_calls = 128
    result = gp_minimize(
        func=objective,
        dimensions=space,
        n_calls=n_calls,
        random_state=0,
        initial_point_generator='sobol',
        n_jobs=-1,
        n_initial_points=16,
        callback=[TqdmCallback(total=n_calls)]
    )

    best_lambda = result.x[0]
    best_div_mult = result.x[1]
    best_max_occurrence = result.x[2]
    best_max_pair_overlap = result.x[3]
    best_risk_aversion = result.x[4]
    best_div_penalty = result.x[5]
    best_div_limit = portfolio_size * best_div_mult
    print("\nBest hyperparameters found (validation):")
    print(f"  lambda_pattern = {best_lambda:.3f}")
    print(f"  diversification_limit = {best_div_limit:.2f} (multiplier = {best_div_mult:.3f})")
    print(f"  max_occurrence = {best_max_occurrence}")
    print(f"  max_pair_overlap = {best_max_pair_overlap}")
    print(f"  risk_aversion = {best_risk_aversion:.4f}")
    print(f"  diversification_penalty = {best_div_penalty:.4f}")

    # Retrain on the full training data (data_train_full) to generate the final portfolio.
    a_full = compute_individual_frequency(data_train_full, num_range=num_range)
    print("Mining frequent patterns on full training data...")
    patterns_full = mine_frequent_patterns_spark(data_train_full, min_support=1e-32)

    profits_train, tot_profit_train, mean_profit_train, var_profit_train, best_portfolio = evaluate_parameters_training(
        a_full, patterns_full, data_train_full, best_lambda, best_div_limit, best_max_occurrence, best_max_pair_overlap, best_risk_aversion, best_div_penalty)
    print(f"Best total profit on full training data: {tot_profit_train:,.2f}")
    print(f"Mean profit (training): {mean_profit_train:,.2f}, Variance: {var_profit_train:,.2f}")

    draws_test_main = data_test.iloc[:, :6].values
    draws_test_extra = data_test.iloc[:, 6].values
    profits_test, total_profit_test, mean_profit_test, var_profit_test = simulate_profit_cpu(
        best_portfolio, draws_test_main, draws_test_extra, prize_ref, ticket_cost)
    t_stat_test, p_val_two_tailed = ttest_1samp(profits_test, 0.0)
    p_val_one_tailed = (p_val_two_tailed / 2) if t_stat_test > 0 else (1 - p_val_two_tailed / 2)

    print("\nHypothesis Testing (One-Sample One-Tailed t-Test) on Testing Data:")
    print(f"  t-statistic: {t_stat_test:.4f}")
    print(f"  One-tailed p-value: {p_val_one_tailed:.4f}")
    if p_val_one_tailed < 0.05:
        print("  Result: The portfolio is significantly profitable on testing data (p < 0.05).")
    else:
        print("  Result: The portfolio is not significantly profitable on testing data (p >= 0.05).")

    print("\nOptimal Lottery Combinations (Best Portfolio):")
    for idx, ticket in enumerate(best_portfolio, start=1):
        print(f"Ticket {idx}: {ticket}")

if __name__ == '__main__':
    main()



